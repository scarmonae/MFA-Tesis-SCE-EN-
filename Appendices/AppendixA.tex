% Appendix A

\section{Parámetros geométricos, espectrales y radiativos de sistemas formadores de imagen}

% 

% \subsection{Parámetros Geométricos}

% \subsubsection{Campo de Visión (FOV)}

% El campo de visión es el ángulo que abarca la escena visible a través del lente de la cámara \cite{Baghdadi2016OpticalMethods}. Es esencial para determinar la extensión de la escena capturada y es fundamental en aplicaciones de teledetección.

% \subsubsection{Profundidad de Campo (DOF)}

% La profundidad de campo se refiere a la distancia sobre el plano de enfoque donde los objetos aparecen nítidos en la imagen \cite{Huang2021ASensing}. Controlar el DOF es crucial para asegurar la calidad de las imágenes capturadas.

% \subsubsection{Campo de Visión Angular}

% El campo de visión angular considera tanto el campo de visión aparente como el real, proporcionando una medida de la cantidad de escena observada a través de la cámara \cite{Dale2013HyperspectralReview}.

% \subsubsection{Distancia de Trabajo}

% La distancia de trabajo es la distancia entre el lente de la cámara y el objeto enfocado \cite{Xue2017SignificantApplications}. Es importante para asegurar el enfoque correcto y la nitidez de la imagen.

% \subsection{Parámetros Espectrales}

% \subsubsection{Sensibilidad Espectral del Sensor}

% La sensibilidad espectral define las longitudes de onda a las que el sensor es sensible y su respuesta relativa \cite{Huang2021ASensing}. Es fundamental para la detección precisa de la reflectancia de la vegetación y el cálculo de índices espectrales como el NDVI.

% \subsubsection{Transmisión Espectral del Lente}

% La transmisión espectral del lente afecta la intensidad y calidad de la luz que llega al sensor \cite{Dale2013HyperspectralReview}. Es esencial evaluar este parámetro para evitar distorsiones espectrales.

% \subsubsection{Características Espectrales de los Filtros}

% Los filtros determinan qué longitudes de onda alcanzan el sensor \cite{Huang2021ASensing}. Analizar su curva de transmitancia espectral es clave para seleccionar bandas espectrales específicas necesarias para los índices de vegetación.

% \subsection{Parámetros Radiométricos}

% Los parámetros radiométricos incluyen la linealidad radiométrica y la sensibilidad del sensor, esenciales para garantizar que la respuesta del sensor sea proporcional a la intensidad de la luz incidente \cite{Zhang2018}.


% \subsection{Función de transferencia de modulación (MTF)}
% \subsubsection{Limitado por difracción}
% \subsubsection{PSF}


\subsection{Spectral Response of RGB Color Cameras}

In the process of acquiring color images using digital RGB cameras, each channel (red, green, and blue) responds to the incoming radiation according to its own \emph{spectral sensitivity}. This response can be modeled linearly or may incorporate some additional nonlinearity, depending on the internal architecture of the camera and the electronic processing applied by the manufacturer \cite{Vora1997DigitalModels,Cheung2004AccurateCameras,Uttner2006SpectralCameras}.

\subsubsection{Linear Response Model}

Let $K$ be the number of channels of the camera (normally, $K=3$, for the red, green, and blue channels). For a given channel $i$, the response of the camera $r_{i}$ is defined as an integral (or discrete sum) of the incident spectral power, filtered by the spectral sensitivity $s_{i}(\lambda)$, during an integration time $t_{\mathrm{integ}}$ \cite{Vora1997DigitalModels,Uttner2006SpectralCameras}:
\begin{equation}
r_{i} \;=\; t_{\mathrm{integ}} \int_{\lambda_{\min}}^{\lambda_{\max}} s_{i}(\lambda)\,I(\lambda)\,\mathrm{d}\lambda \;+\; n_{i},
\label{eq:linear_response}
\end{equation}
where:
\begin{itemize}
    \item $s_{i}(\lambda)$ is the spectral sensitivity of channel $i$ (e.g., R, G, or B),
    \item $I(\lambda)$ represents the spectral power distribution of the incoming light (including both the light source and the reflectance of the object at each wavelength),
    \item $t_{\mathrm{integ}}$ is the integration time of the camera (a scalar factor),
    \item $n_{i}$ is a noise term (dark offset, read noise, etc.).
\end{itemize}

In cameras whose sensor is strictly linear and in which the manufacturer does not apply any nonlinear processing to the signal, Equation~\eqref{eq:linear_response} can adequately represent the response \cite{Vora1997DigitalModels}. Under this assumption, the digital value read by the camera increases proportionally to the light intensity, provided that the saturation region is not reached and that the noise remains relatively constant.

\subsubsection{Response Model with Static Nonlinearity}

In many cases, the manufacturer implements a \emph{gamma curve} or some other nonlinear mapping to compress the dynamic range or compensate for subsystem responses. In such cases, the response is modeled as a monotonically increasing function $F(\cdot)$ acting on the internal linear signal \cite{Vora1997DigitalModels,Cheung2004AccurateCameras}:
\begin{equation}
r_{i} \;=\; F\!\Bigl(\,t_{\mathrm{integ}}\!\!\int_{\lambda_{\min}}^{\lambda_{\max}} s_{i}(\lambda)\,I(\lambda)\,\mathrm{d}\lambda \;+\; n_{i}\Bigr).
\label{eq:nonlinear_response}
\end{equation}
In practice, a power-law model can be used to approximately describe the nonlinearity:
\begin{equation}
r_{i} \;=\; \bigl(\,\rho_{i}\bigr)^{\,\gamma_{i}},
\quad
\text{with}
\quad
\rho_{i} \;=\; t_{\mathrm{integ}}\!\!\int_{\lambda_{\min}}^{\lambda_{\max}} s_{i}(\lambda)\,I(\lambda)\,\mathrm{d}\lambda \;+\; n_{i}.
\end{equation}
The exponent $\gamma_{i}$ depends on the specific camera and channel; sometimes it is assumed that all channels share the same $\gamma$. To \emph{linearize} the measurements, the inverse is applied:
\begin{equation}
\rho_{i} \;=\; \bigl(r_{i}\bigr)^{\frac{1}{\gamma_{i}}}.
\end{equation}
In this way, a signal proportional to the actual energy incident on the sensor is recovered, which is essential for tasks such as calibration, color balancing, and estimation of spectral sensitivity \cite{Cheung2004AccurateCameras,Uttner2006SpectralCameras}.

\subsubsection{Spectral Sensitivity Characterization}

% The \emph{spectral sensitivity} $s_{i}(\lambda)$ represents the efficiency with which channel $i$ detects radiation at wavelength $\lambda$. In an RGB camera, there are typically three curves $s_{R}(\lambda)$, $s_{G}(\lambda)$, and $s_{B}(\lambda)$. Knowing these curves allows:
% \begin{enumerate}
%     \item Accurate computation of the camera response to a given spectrum $I(\lambda)$, even for illuminants and objects not considered during initial calibration.
%     \item More robust colorimetric transformations (e.g., from RGB to spaces like CIE XYZ), especially when compensating for variations in the illuminant \cite{vora1997,cheung2004,buettner2006}.
% \end{enumerate}

There are direct methods, such as illuminating the camera with monochromatic light and measuring the response at each $\lambda$, but these are often costly and impractical. Therefore, \emph{indirect estimation} techniques have been developed to estimate spectral sensitivity using a set of known filters or color patches under a measured illumination \cite{Uttner2006SpectralCameras}. These use equations of the form:
\begin{equation}
R \;=\; t_{\mathrm{integ}} \;\mathbf{s}^{T}\;\mathbf{C},
\label{eq:matrix_equation}
\end{equation}
where: $\mathbf{C}$ contains the contribution of the filters and the illuminant in matrix form; $\mathbf{s}$ is the matrix containing, in its columns, the $s_{i}(\lambda)$ for each channel; and $R$ is the matrix of measured responses for each filter.

Usually, the matrix $\mathbf{C}$ is poorly conditioned because the filters used for characterization often have overlapping spectra, which introduces some dependency among the matrix columns. Using inversion techniques (or constrained quadratic programming), one can solve Equation~\eqref{eq:matrix_equation} to find $\mathbf{s}$ \cite{Uttner2006SpectralCameras}.

Several authors have emphasized the importance of correcting nonlinearity before attempting color calibrations. In particular, \cite{Cheung2004AccurateCameras} shows that failing to properly linearize the response can significantly increase errors in colorimetric characterization. Similarly, \cite{Uttner2006SpectralCameras} points out that knowing the spectral sensitivity curve (and having linearized data) helps to develop more robust \emph{color constancy} and color correction algorithms. Therefore, the models described in Equations~(\ref{eq:linear_response}, \ref{eq:nonlinear_response}) form the theoretical foundation for understanding signal formation in RGB cameras.

\subsection{Luminance in Optical Systems}

\textit{Luminance} is the luminous intensity per unit area in a given direction, expressed in \textit{candelas per square meter} (cd/m²). It is defined as:

\begin{equation}
L = \frac{d^2\Phi}{dA \cos\theta d\Omega}
\end{equation}

where \( d^2\Phi \) is the luminous power, \( dA \) is the area of the source, \( d\Omega \) is the solid angle, and \( \theta \) is the angle of incidence. In optical systems, luminance is key to evaluating radiometric response and image quality \cite{Wuller2007TheMeters}.

The use of digital cameras to measure luminance requires calibration and control of variables such as exposure and spectral response. By applying appropriate procedures, accurate and reproducible measurements can be obtained.

\subsection{Contrast in an Optical System}

Contrast is a fundamental property in evaluating the performance of optical systems, as it determines the system's ability to distinguish differences in light intensity in an image. In physical terms, contrast is defined as the relative variation in luminance between the bright and dark regions of an object or image \cite{Boreman2001ModulationTT52}.

\subsubsection{Mathematical Definition of Contrast}

Contrast is generally expressed in terms of \textit{modulation depth}, defined as:

\begin{equation}
M = \frac{A_{\max} - A_{\min}}{A_{\max} + A_{\min}}
\label{eq:contraste}
\end{equation}

where:
\begin{itemize}
    \item $A_{\max}$ is the maximum intensity in the image.
    \item $A_{\min}$ is the minimum intensity in the image.
\end{itemize}

When $A_{\min} = 0$, the contrast is maximum $(M = 1)$. Conversely, if $A_{\max} = A_{\min}$, the modulation is null $(M = 0)$, implying that the image lacks intensity variations and, therefore, visual detail.

\subsubsection{Relation with the Modulation Transfer Function (MTF)}

Contrast in an optical system is directly related to the \textit{Modulation Transfer Function} (MTF), which measures how modulation is preserved in the image across different spatial frequencies. The MTF is defined as:

\begin{equation}
\text{MTF}(f) = \frac{M_{\text{image}}(f)}{M_{\text{object}}(f)}
\label{eq:mtf}
\end{equation}

where:
\begin{itemize}
    \item $M_{\text{image}}(f)$ is the measured modulation in the image for a spatial frequency $f$.
    \item $M_{\text{object}}(f)$ is the modulation of the object at the same frequency.
\end{itemize}

At low spatial frequencies, the MTF is close to unity, indicating that the object's contrast is preserved in the image. As the spatial frequency increases, the MTF decreases due to the limitations of the optical system, such as diffraction, aberrations, and noise.\\

Contrast is a key parameter in evaluating image quality, as it directly influences an optical system’s ability to reproduce details faithfully. Its impact on visual perception and optical system design is significant, especially in applications where resolution and sharpness are essential. Generally, contrast determines the differentiation between regions of different luminance in an image, affecting the perception of fine details and the visual quality in optical and electro-optical systems.\\

An adequate level of contrast is crucial for correctly identifying low-contrast structures in scientific and medical images. In applications such as microscopy, radiography, and remote sensing, differentiating between regions of similar intensity is essential for extracting relevant information. The ability to detect subtle variations in light intensity can influence the analysis of biomedical images, the identification of structures in astronomical images, and the classification of surfaces in satellite imagery.\\

In addition to its impact on visual perception, contrast plays a key role in optimizing optical system design. Evaluating contrast through the modulation transfer function (MTF) allows designing and optimizing optical systems for specific applications. In fields such as astronomy and precision optics, an appropriate MTF ensures that optical systems can resolve fine structures without significant loss of information. This is particularly important in applications requiring high fidelity in detail transfer, such as designing lenses for scientific photography, surface inspection, and image reconstruction in computed tomography.\\

From a visual perception perspective, the human eye's sensitivity varies with spatial frequency, being most efficient at detecting intermediate contrasts. Therefore, an optimized optical design must ensure that the MTF maximizes contrast transfer in the range of spatial frequencies most relevant to human vision. This consideration is essential in display systems, digital cameras, and augmented reality devices, where the perceived image quality directly depends on correct contrast reproduction.\\

Contrast not only determines an optical system’s ability to resolve fine details but also conditions the design of optical components and their applicability in various scientific and technological fields. Proper characterization of contrast using the MTF and other metrics is essential to ensure optimal performance of imaging systems, guaranteeing a faithful representation of captured information.\\


\subsection{Image Formation in Optical Systems}

Image formation in optical systems can be mathematically described as a convolution operation between the spatial irradiance distribution of the object and the impulse response of the optical system. This operation captures how an object is transformed into the final image generated by the system. Formally, the image \( g(x,y) \) is obtained by:

\begin{equation}
g(x,y) = f(x,y) * h(x,y),
\end{equation}

where \( f(x,y) \) is the spatial irradiance of the ideal object and \( h(x,y) \) is the impulse response of the optical system, which characterizes the combined effects of physical phenomena such as diffraction and aberrations on the final image.

In an ideal optical system, the impulse response is a delta function \(\delta(x,y)\), resulting in an exact replica of the object. However, in real systems, the impulse response has a finite spatial distribution due to diffraction and aberration effects, causing even point sources to appear as blurred spots in the image, known as \textit{blur spots} \cite{Boreman2001ModulationTT52}. This actual distribution, known as the Point Spread Function (PSF), defines the smallest detail that can be resolved by the optical system.

The convolution process requires the system to meet the properties of linearity and spatial invariance (LSI), implying that the functional form of \( h(x,y) \) does not change with position in the image plane. Although these conditions are only approximated in real systems due to field-angle-dependent aberrations, the convolutional analysis remains valid within isoplanatic regions, where the PSF stays approximately constant \cite{Boreman2001ModulationTT52}.

A continuous function \( f(x_{\text{obj}},y_{\text{obj}}) \) can be discretized using the sampling property of the Dirac delta function as:

\begin{equation}
f(x',y') = \iint f(x_{\text{obj}},y_{\text{obj}}) \delta(x'-x_{\text{obj}}, y'-y_{\text{obj}})\,dx_{\text{obj}}dy_{\text{obj}}.
\end{equation}

The convolution of this discretized function with the impulse response \( h(x',y') \) of the optical system can be physically interpreted as placing a copy of the impulse response centered at each discrete point \((x', y')\), weighted by the object irradiance at that point, and then summing all these individual contributions to generate the resulting image. Mathematically, this process is equivalent to the convolution:

\begin{equation}
g(x,y) = \iint h(x-x', y-y') f(x',y')\, dx'dy'.
\end{equation}

% \begin{figure}
%     \centering
%     % \includegraphics[width=0.5\linewidth]{}
%     \caption{Add image showing the convolution of the object with the impulse response to generate the image.}
%     \label{fig:enter-label}
% \end{figure}

\subsection{Spatial Frequency Response of Optical Systems}

The spectral response of an optical system describes its behavior with respect to different spatial and spectral frequencies. In this context, the concept of \textit{impulse response} is fundamental, since any image can be considered as a weighted combination of responses to point sources. This section develops the mathematical framework to model light propagation in an optical system through various key functions \cite{Boreman2001ModulationTT52}.

\subsubsection{Impulse Response and the Point Spread Function (PSF)}

The \textit{impulse response} of an optical system describes how it responds to a point source. Mathematically, a point source is modeled using the Dirac delta function:

\begin{equation}
f(x,y) = \delta(x - x_0, y - y_0),
\end{equation}

where $(x_0, y_0)$ represents the position of the point in the object plane. In an ideal system without aberrations or diffraction, the image of this source would be another point. However, in practice, the optical system produces an irradiance distribution known as the \textit{Point Spread Function} (PSF):

\begin{equation}
g(x,y) = h(x,y) = \text{PSF}(x,y),
\end{equation}

where $h(x,y)$ represents the impulse response of the system. The PSF describes how a point is spread in the image plane due to effects such as diffraction and optical aberrations.

The PSF is related to the Optical Transfer Function (OTF) through the Fourier transform:

\begin{equation}
\text{OTF}(u,v) = \mathcal{F} \{ \text{PSF}(x,y) \}.
\end{equation}

\subsubsection{Line Spread Function (LSF)}

The \textit{Line Spread Function} (LSF) is a function derived from the PSF and describes the response of the optical system when the excitation is a thin line instead of a point. It is obtained by integrating the PSF along one of its dimensions:

\begin{equation}
\text{LSF}(x) = \int_{-\infty}^{\infty} \text{PSF}(x,y) \, dy.
\end{equation}

This function is used in the characterization of optical systems, as it allows the study of blur propagation in a specific direction.

\subsubsection{Edge Spread Function (ESF)}

The \textit{Edge Spread Function} (ESF) models the system's response to an abrupt edge transition. This function is particularly useful in the experimental measurement of the MTF, as it can be easily obtained from images of high-contrast edges. The ESF is related to the LSF through the derivative:

\begin{equation}
\text{LSF}(x) = \frac{d}{dx} \text{ESF}(x).
\end{equation}

This relationship allows determining the sharpness of an optical system from images of well-defined edges.

\subsubsection{Modulation Transfer Function (MTF)}

The \textit{Modulation Transfer Function} (MTF) characterizes the ability of an optical system to transfer contrast of different spatial frequencies from the object to the image. It is obtained from the Fourier transform of the LSF:

\begin{equation}
\text{MTF}(u) = \left| \mathcal{F} \{ \text{LSF}(x) \} \right|.
\end{equation}

The MTF indicates the resolving power of the system: a value close to 1 implies perfect contrast transfer, while values close to 0 indicate total loss of detail at those spatial frequencies. The formal definition of contrast or modulation is expressed as:

\begin{equation}
M = \frac{A_{\max} - A_{\min}}{A_{\max} + A_{\min}} = \frac{ac}{dc},
\end{equation}

where the terms refer respectively to the amplitude of the irradiance variation (variable sinusoidal component) and the mean level (bias).

It is important to note that the MTF generally decreases with spatial frequency, which reflects the reduction in the system’s ability to reproduce fine details.

\subsubsection{Additional Considerations on the OTF and System Linearity}

A relevant extension of the above lies in the distinction between the \textit{Optical Transfer Function} (OTF) and the \textit{Modulation Transfer Function} (MTF). Although in many applications direct reference is made to the MTF, the OTF is, in general terms, a complex function that can be expressed as

\begin{equation}
\text{OTF}(f) \;=\; \text{MTF}(f)\,\exp\bigl[-\,j\,\text{PTF}(f)\bigr],
\end{equation}

where $\text{MTF}(f)$ represents the magnitude of the OTF and $\text{PTF}(f)$ describes the phase associated with the transfer of spatial frequencies \cite{Boreman2001ModulationTT52}. This phase can introduce changes in the shape of the wave function in the image, generating, for example, contrast inversions or shifts in the position of the distribution’s maximum. In a system with good symmetry and no significant aberrations, the phase tends to be zero or constant, but in cases of aberrations such as coma or astigmatism, the PTF becomes more complex and affects the fidelity with which fine details are reproduced.

Another aspect that complements the study of the MTF is the linear and shift-invariant (LSI) nature of the system. The formulation through the convolution

\begin{equation}
g(x,y)\;=\;f(x,y)\,*\,h(x,y)
\end{equation}

is strictly valid when the system is linear in irradiance and when its \textit{Point Spread Function} (PSF) does not depend on the position in the image plane. These assumptions hold to a good approximation in incoherent optical systems with moderate aberrations, but may fail if the system operates outside its linear regimes or if aberrations vary drastically with field angle \cite{Boreman2001ModulationTT52}. This observation explains the importance of the so-called \textit{isoplanaticity}, which delimits regions where shift invariance is preserved and allows the convolution model to be applied without loss of accuracy.\\

The practical relevance of the MTF becomes evident when analyzing the resolution and sensitivity of the system for different values of spatial frequency. Although a single number is sometimes used to represent resolution (for example, the spatial frequency at which the MTF drops to a certain threshold), it is more informative to have the complete MTF curve as a function of frequency. This makes it possible to determine whether a system maintains contrast across the entire range of spatial frequencies of interest, or if its performance is limited in intermediate or high frequencies. Additionally, in a system composed of multiple subsystems, the total MTF is obtained as the product of the MTFs of each element in the image formation chain, which helps identify where the greatest contrast losses occur.\\

Finally, the notion of resolution should not be restricted to the simple separation between two distinguishable points, but rather, following Boreman's criterion \cite{Boreman2001ModulationTT52}, should be evaluated through the integral behavior of the MTF. Especially in applications such as microscopy, astronomical observation, or remote sensing, it is not enough to reach a high cutoff frequency; solid performance in intermediate frequencies is also essential to ensure the detection of low-contrast structures. This is directly related to the characterization of \textit{image quality} and to the possibility of optimizing optical design by maximizing the MTF in the range of spatial frequencies most relevant to each application.\\

In summary, the OTF approach and its magnitude, the MTF, provide a comprehensive framework to understand and quantify how contrast and resolution are affected by diffraction, aberrations, and the intrinsic limitations of the system. Phase details (PTF) can become decisive when analyzing the distortion of shapes and the displacement of irradiance maxima, while linearity and shift invariance provide the foundations for these frequency-domain descriptions to be rigorous and applicable in a wide range of optical systems \cite{Boreman2001ModulationTT52}.\\

\subsubsection{Phase Transfer Function (PTF)}

The \textit{Phase Transfer Function} (PTF) describes the phase shift introduced by the optical system as a function of spatial frequency. It is formally defined as the argument part of the OTF:

\begin{equation}
\text{PTF}(u,v) = \arg{\text{OTF}(u,v)}.
\end{equation}

In ideal and symmetric systems, the PTF can take very simple values (0 or $\pi$), indicating very basic phase changes. However, in real systems that present optical aberrations such as coma, astigmatism, or focal shifts, the PTF exhibits more complex nonlinear behaviors. These nonlinearities introduce significant distortions in the final image, altering not only the position but also the shape of the object’s details.

An important property of the PTF occurs when the MTF passes through zero: at that point, the PTF experiences an abrupt change of $\pi$, known as phase inversion. This is visually manifested in radial test patterns as a sudden switch from white to black, clearly perceptible and useful for experimentally evaluating the properties of the system.

\subsubsection{Relationship Between the Response Functions}

The image formation process in an optical system can be understood as a convolution between the object irradiance and the PSF:

\begin{equation}
g(x,y) = f(x,y) * h(x,y).
\end{equation}

If the system is linear and time-invariant, this convolution in the spatial domain becomes a multiplication in the spatial frequency domain:

\begin{equation}
G(u,v) = F(u,v) \cdot H(u,v),
\end{equation}

where:
\begin{itemize}
    \item $G(u,v)$ is the Fourier transform of the resulting image.
    \item $F(u,v)$ is the Fourier transform of the object.
    \item $H(u,v) = \text{OTF}(u,v)$ is the optical transfer function of the system.
\end{itemize}

The spectral response of an optical system is defined through the impulse response and derived functions such as the PSF, LSF, and ESF. These functions allow modeling how an optical system processes light and affect the quality of the final image. The MTF, obtained from the Fourier transform of the LSF, provides an essential quantitative criterion for evaluating the resolution of an optical system.

\subsection{Relationship Between MTF and the Resolution of Optical Systems}

The relationship between the \textit{Modulation Transfer Function} (MTF) and the resolution of an optical system is essential for understanding how these systems transfer object details to the image. Although resolution is commonly specified as the minimum discernible distance between two points or lines in object space or the image plane, a more complete and rigorous description of resolution can be made using the MTF.

In the spatial domain, resolution can be defined as the minimum distance \(\Delta x\) at which two points can be identified as separate:

\begin{equation}
\Delta x \quad \text{or} \quad \Delta \theta
\end{equation}

However, in the spatial frequency domain, resolution can be specified by the maximum frequency \(f_{\text{max}}\), known as the cutoff frequency, for which the MTF drops below a certain defined threshold:

\begin{equation}
\text{MTF}(f_{\text{max}}) = \text{specified threshold}
\end{equation}

This maximum frequency is usually measured in cycles per millimeter (cy/mm) or cycles per milliradian (cy/mrad). The main advantage of the MTF-based approach lies in its ability to provide information about how the system transfers contrast across a broad range of spatial frequencies, not only at the cutoff frequency.

Moreover, the concept of \textit{Noise Equivalent Modulation} (NEM) can be introduced, which represents the minimum modulation discernible above the inherent noise of the system. The frequency at which the MTF curve intersects the NEM curve can be considered a practical criterion for determining the system’s effective resolution:

\begin{equation}
\text{MTF}(f_{\text{NEM}}) = \text{NEM}
\end{equation}

Nonetheless, specifying only the cutoff frequency as an indicator of resolution may be insufficient or even misleading. Two systems with identical cutoff resolution frequencies may exhibit substantial differences in image quality due to variations in their performance at intermediate and low spatial frequencies. Therefore, the concept of \textit{MTF Area} (MTFA) is introduced, defined as the area between the MTF curve and the NEM curve within the range of relevant spatial frequencies:

\begin{equation}
\text{MTFA} = \int_{f_{\text{min}}}^{f_{\text{max}}} \left[\text{MTF}(f) - \text{NEM}(f)\right] df
\end{equation}

Maximizing the MTFA implies optimizing system performance across all relevant spatial frequencies, thus ensuring a faithful and detailed reproduction of the object in the final image. In conclusion, evaluating resolution using the MTF provides a more complete and meaningful characterization of the performance of optical systems, especially in critical applications that require high image quality and precision.

\subsection{Image Quality Factors}

The evaluation of image quality in digital capture systems is fundamental in the field of applied physics, as it allows determining the performance and suitability of these systems for specific applications. Various factors contribute to image quality, and their detailed analysis is essential for a comprehensive understanding of the system. The main image quality factors are described below:

% Please add the following required packages to your document preamble:
% \usepackage{longtable}
% Note: It may be necessary to compile the document several times to get a multi-page table to line up properly
\begin{longtable}[c]{|c|c|c|}
\hline
\textbf{Factor} &
  \textbf{Description} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Graph \\ Used\end{tabular}} \\ \hline
\endfirsthead
%
\endhead
%
\textbf{Sharpness} &
  \begin{tabular}[c]{@{}c@{}}Measure of the system’s ability \\ to resolve fine details. Evaluated \\ using the Spatial Frequency \\ Response (SFR) \\ \cite{Imatest2025Sharpness}.\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Slanted-edge, \\ ISO 12233\end{tabular} \\ \hline
\textbf{Noise} &
  \begin{tabular}[c]{@{}c@{}}Random variations in brightness \\ or color that degrade the image. \\ Depends on ISO sensitivity and \\ lighting conditions.\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Step charts, \\ ColorChecker\end{tabular} \\ \hline
\textbf{Dynamic Range} &
  \begin{tabular}[c]{@{}c@{}}System’s ability to capture details \\ in both shadows and highlights \\ simultaneously.\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Grayscale step \\ charts \\ (transmissive and \\ reflective)\end{tabular} \\ \hline
\textbf{Distortion} &
  \begin{tabular}[c]{@{}c@{}}Geometric alterations in the image \\ such as barrel or pincushion \\ distortion.\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Rectangular grid \\ or Checkerboard\end{tabular} \\ \hline
\textbf{Chromatic Aberration} &
  \begin{tabular}[c]{@{}c@{}}Color fringing at the edges of \\ objects due to wavelength-\\ dependent refraction.\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Dot pattern, \\ SFRplus\end{tabular} \\ \hline
\textbf{Uniformity} &
  \begin{tabular}[c]{@{}c@{}}Consistency of brightness and \\ color in the image, affected by \\ vignetting or uneven lighting.\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Uniformly \\ illuminated \\ surface\end{tabular} \\ \hline
\textbf{Artifacts} &
  \begin{tabular}[c]{@{}c@{}}Image anomalies such as aliasing, \\ moiré, or excessive compression.\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Wedges in \\ ISO 12233, \\ Log F-Contrast\end{tabular} \\ \hline
\textbf{Sensitivity} &
  \begin{tabular}[c]{@{}c@{}}Capture capability under low-light \\ conditions without excessive noise.\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Step charts, \\ ColorChecker\end{tabular} \\ \hline
\textbf{Color Reproduction} &
  \begin{tabular}[c]{@{}c@{}}Accuracy in capturing and \\ representing the original scene \\ colors.\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}X-Rite \\ ColorChecker, \\ IT8.7\end{tabular} \\ \hline
\textbf{Internal Reflection (Flare)} &
  \begin{tabular}[c]{@{}c@{}}Light scattering within the optical \\ system that reduces contrast.\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Q-13 or Q-14 \\ step chart, \\ white with \\ “black hole”\end{tabular} \\ \hline
\textbf{Texture Detail} &
  \begin{tabular}[c]{@{}c@{}}Ability to preserve fine details \\ without excessive smoothing.\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Log F-Contrast, \\ Dead Leaves \\ (Spilled Coins)\end{tabular} \\ \hline
\textbf{Flicker} &
  \begin{tabular}[c]{@{}c@{}}Temporal variations in image \\ brightness caused by intermittent \\ lighting.\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Step charts, \\ eSFR ISO\end{tabular} \\ \hline
\textbf{Color Moiré} &
  \begin{tabular}[c]{@{}c@{}}Color interference patterns in \\ fine details due to the sensor’s \\ structure.\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Wedges in \\ ISO 12233, \\ Log F-Contrast\end{tabular} \\ \hline
\textbf{Blemishes} &
  \begin{tabular}[c]{@{}c@{}}Image defects caused by dead \\ pixels, dust, or sensor \\ imperfections.\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Uniformly \\ illuminated \\ surface\end{tabular} \\ \hline
\caption{Image Quality Factors and graphs used for their measurement, based on \textit{Imatest} image quality evaluation standards and ISO 12233. Adapted from \cite{ImatestTeam2025ImageFactors}.}
\label{tab:factores_calidad}\\
\end{longtable}